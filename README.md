# Pac-Man RL â€” Language Model Strategy Training

![Ms. Pac-Man gameplay](https://ale.farama.org/_images/ms_pacman.gif)

Training language models to play Pac-Man by generating strategy code instead of discrete actions.

## Watch the Trained Agent Play

<video src="https://github.com/user-attachments/assets/e3310ca1-fcfe-4bdf-ac76-55333646bb13" controls></video>

**ðŸ¤— Trained Models:**
- [justinj92/Llama-3.1-8B-Pacman-Player](https://huggingface.co/justinj92/Llama-3.1-8B-Pacman-Player) (Llama 3.1 8B)
- [justinj92/gpt-oss-20B-pacmanplayer](https://huggingface.co/justinj92/gpt-oss-20B-pacmanplayer) (GPT-OSS 20B)

## What Is This?

A reinforcement learning environment where LLMs learn to write Python functions that control Pac-Man. Instead of outputting action IDs directly, the model generates code like:

```python
def strategy(obs):
    if obs.frightened_timer > 0: return [1, 2, 0]  # Chase ghosts
    if obs.nearest_ghost_distance < 3: return [3]  # Flee left
    return [0, 1, 2, 3]  # Explore
```

The environment validates the code, executes it safely, and uses the trajectory rewards to fine-tune the model with GRPO.

**Why this matters**: Tests whether LLMs can learn goal-directed reasoning through code generation, not just memorization.

## Training Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ENVIRONMENT â†’ Structured Observation                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Pac-Man position, lives, score                       â”‚
â”‚  â€¢ Ghost positions/states (normal/frightened)           â”‚
â”‚  â€¢ Pellet counts, nearest distances                     â”‚
â”‚  â€¢ Safe directions, legal actions                       â”‚
â”‚  â€¢ 15x15 maze layout (walls, pellets, power-ups)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
           observation_to_prompt(obs)
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. PROMPT GENERATION (~500 tokens)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Write a compact Ms. Pac-Man strategy function.         â”‚
â”‚                                                          â”‚
â”‚  obs: pacman_position, ghost_positions, ghost_states,   â”‚
â”‚       frightened_timer, nearest_ghost_distance,         â”‚
â”‚       safe_directions, legal_actions, pellets, lives    â”‚
â”‚                                                          â”‚
â”‚  Return list of 1-5 actions [0,1,2,3,4]                 â”‚
â”‚  (UP,RIGHT,DOWN,LEFT,STAY)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              Llama 3.1 8B Instruct (LoRA)
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. LLM GENERATES STRATEGY CODE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  def strategy(obs):                                     â”‚
â”‚      if obs.frightened_timer > 0:                       â”‚
â”‚          return [1, 2, 0]  # Chase ghosts               â”‚
â”‚      if obs.nearest_ghost_distance < 3:                 â”‚
â”‚          if "left" in obs.safe_directions: return [3]   â”‚
â”‚      return [0, 1, 2, 3]  # Explore                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
       execute_with_time_limit(code, obs)
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. EXECUTE â†’ ROLLOUT â†’ REWARDS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Validate code (AST parse, sandbox execution)         â”‚
â”‚  â€¢ Run 12-step rollout in Pac-Man environment           â”‚
â”‚  â€¢ Collect rewards: +10 pellet, +50 power, +200 ghost   â”‚
â”‚  â€¢ Calculate trajectory return                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
            After batch of episodes...
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. GRPO UPDATE â†’ Improve Strategy                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Compute advantages from trajectory rewards           â”‚
â”‚  â€¢ Backpropagate through LoRA adapters                  â”‚
â”‚  â€¢ Model learns: avoid ghosts, chase power pellets,     â”‚
â”‚    maximize score, survive longer                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Technical Stack:**

- **Base Model**: [Llama 3.1 8B Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) (LoRA rank 32, alpha 64)
- **Trained Models**: 
  - [ðŸ¤— Llama-3.1-8B-Pacman-Player](https://huggingface.co/justinj92/Llama-3.1-8B-Pacman-Player) (8B)
  - [ðŸ¤— gpt-oss-20B-pacmanplayer](https://huggingface.co/justinj92/gpt-oss-20B-pacmanplayer) (20B)
- **Training**: GRPO via TRL + Unsloth
- **Environment**: Docker-based FastAPI server (configurable difficulty, ghost AI)
- **Observation**: ~500 token prompts with structured game state

## Project Structure

```text
pacman_env/               # OpenEnv-compatible environment
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ pacman_environment.py   # Game logic, rewards, collision detection
â”‚   â””â”€â”€ app.py                   # FastAPI service (Docker-ready)
â”œâ”€â”€ maze_generator.py     # Procedural maze generation
â”œâ”€â”€ ghost_ai.py           # Ghost personalities (chase, ambush, scatter)
â”œâ”€â”€ client.py             # HTTP client with Docker orchestration
â””â”€â”€ models.py             # Action/Observation/State types

play_the_game/            # Inference and visualization
â”œâ”€â”€ html_pacman_player.py       # Flask web UI for watching trained agents
â””â”€â”€ simple_model_server.py      # Lightweight model serving

train_pacman_docker_grpo_v2.py  # Main training script
```

## Installation

This project requires [Meta's OpenEnv framework](https://github.com/meta-pytorch/OpenEnv). See [INSTALL.md](INSTALL.md) for detailed setup instructions.

**Quick install:**

```bash
# 1. Install OpenEnv
git clone https://github.com/meta-pytorch/OpenEnv.git
cd OpenEnv && pip install -e . && cd ..

# 2. Clone this repo and install dependencies
git clone https://github.com/cpich3g/pacman-rl.git
cd pacman-rl
pip install -r requirements.txt

# 3. Register pacman_env with OpenEnv
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# 4. Pull Docker image
docker pull ghcr.io/meta-pytorch/openenv-pacman-env:latest
```

## Quick Start

**Explore the notebook:**

Check out [notebooks/Pacman-RL.ipynb](notebooks/Pacman-RL.ipynb) for interactive examples.

**Use the pretrained models:**

Download from ðŸ¤— Hugging Face (choose one):

**Option 1: Llama 3.1 8B**
```bash
git clone https://huggingface.co/justinj92/Llama-3.1-8B-Pacman-Player
python play_the_game/simple_model_server.py --model-path Llama-3.1-8B-Pacman-Player
```

**Option 2: GPT-OSS 20B (larger, better performance)**
```bash
git clone https://huggingface.co/justinj92/gpt-oss-20B-pacmanplayer
python play_the_game/simple_model_server.py --model-path gpt-oss-20B-pacmanplayer
```

**Then launch the web UI:**
```bash
python play_the_game/html_pacman_player.py
# Open browser to http://localhost:5000
```

**Train from scratch:**

```bash
# Launches Docker container + GRPO training
python train_pacman_docker_grpo_v2.py
```

**Watch your trained agent play:**

```bash
# Start model server with your checkpoint
python play_the_game/simple_model_server.py --model-path outputs_pacman/final_model

# Launch web UI (separate terminal)
python play_the_game/html_pacman_player.py
# Open browser to http://localhost:5000
```

## Why This Design?

**Code Generation vs Action Prediction:**

- Traditional RL: Model outputs action ID directly â†’ limited interpretability
- This approach: Model writes strategy function â†’ explainable, debuggable, composable

**Long-Horizon Reasoning:**

- 12-step rollouts test planning beyond immediate rewards
- Sparse rewards (power pellets, level completion) require multi-step thinking

**Emergent Strategies:**

- Early training: Random exploration, high death rate
- Mid training: Learns to avoid ghosts, collect nearby pellets
- Late training: Coordinates power pellet usage with ghost hunting

## Environment Details

**Configurable Parameters:**

- `PACMAN_DIFFICULTY`: 1-5 (maze size, ghost count, AI aggression)
- `PACMAN_GHOST_AI`: random | heuristic (Blinky/Pinky/Inky/Clyde personalities)
- `PACMAN_MAZE_SIZE`: Grid dimensions (default 15x15)
- `PACMAN_MAX_STEPS`: Episode length (default 600)

**Reward Structure:**

- Pellet: +10
- Power pellet: +50
- Eating ghost (when powered): +200
- Death: -500
- Level complete: +1000
- Time penalty: -1 per step

## Training Progress (Sample)

```text
Step 50  | Reward: +8.7  | Turns: 134  | Parse failures: 34% â†’ 12%
Step 100 | Reward: +22.3 | Turns: 189  | Code quality improving
Step 200 | Reward: +45.6 | Turns: 243  | Power pellet strategies emerge
Step 400 | Reward: +78.2 | Turns: 312  | Coordinated ghost evasion
```

**Learned Behaviors:**

- Flee from ghosts when `nearest_ghost_distance < 3`
- Chase ghosts during `frightened_timer > 0`
- Prefer `safe_directions` over risky moves
- Prioritize power pellets when ghosts nearby

## Requirements

- Python 3.11+
- [OpenEnv](https://github.com/meta-pytorch/OpenEnv) (Meta's environment framework)
- PyTorch, Unsloth, TRL, Datasets
- Docker (for environment server)
- GPU recommended (4-bit quantization supported)

**Full dependency list:** See [requirements.txt](requirements.txt)

**Installation guide:** See [INSTALL.md](INSTALL.md)

## Pretrained Models

Two trained models are available on Hugging Face:

### ðŸ¤— Llama 3.1 8B Model

**[justinj92/Llama-3.1-8B-Pacman-Player](https://huggingface.co/justinj92/Llama-3.1-8B-Pacman-Player)**

**Details:**
- Base: Meta-Llama-3.1-8B-Instruct
- Fine-tuning: GRPO with code generation
- LoRA: rank 32, alpha 64
- Training: 400 steps on Pac-Man gameplay

**Quick Use:**
```bash
git clone https://huggingface.co/justinj92/Llama-3.1-8B-Pacman-Player
python play_the_game/simple_model_server.py --model-path Llama-3.1-8B-Pacman-Player
```

### ðŸ¤— GPT-OSS 20B Model

**[justinj92/gpt-oss-20B-pacmanplayer](https://huggingface.co/justinj92/gpt-oss-20B-pacmanplayer)**

**Details:**
- Base: GPT-OSS 20B
- Fine-tuning: GRPO with code generation
- Larger model with potentially better performance

**Quick Use:**
```bash
git clone https://huggingface.co/justinj92/gpt-oss-20B-pacmanplayer
python play_the_game/simple_model_server.py --model-path gpt-oss-20B-pacmanplayer
```

## Installation as Package

```bash
# Install in development mode
pip install -e .

# Or install normally
pip install .
```
